{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from Parser import *\n",
    "from Classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DictionaryUtils:\n",
    "    #Append new data to existing dictionary/frequency arrays\n",
    "    @staticmethod\n",
    "    def createDictionary(dictionary, frequency, files):\n",
    "        tmpDict = []\n",
    "        tmpFreq = []\n",
    "        tmpAppends = []\n",
    "                \n",
    "        #Create small dict\n",
    "        for file in files:\n",
    "            for word in file.split(\" \"):\n",
    "                if word in tmpDict:\n",
    "                    tmpFreq[tmpDict.index(word)] += 1\n",
    "                else:\n",
    "                    tmpDict.append(word)\n",
    "                    tmpFreq.append(1)\n",
    "\n",
    "        #Merge old and new dict\n",
    "        for i in xrange(len(tmpDict)):\n",
    "            if tmpDict[i] in dictionary:\n",
    "                frequency[dictionary.index(tmpDict[i])] += tmpFreq[i]\n",
    "            else:\n",
    "                tmpAppends.append(tmpDict[i])\n",
    "                frequency.append(tmpFreq[i])\n",
    "                \n",
    "        return dictionary + tmpAppends, frequency\n",
    "    \n",
    "    #Cut the extremes off the dictionary array\n",
    "    @staticmethod\n",
    "    def simplifyDictionary(dictionary, frequency, percentageUpperBound = 0.95, minOccurences = 200):\n",
    "        newDict = []\n",
    "        maximum = max(frequency)\n",
    "        for i in xrange(len(dictionary)):\n",
    "            if frequency[i] <= maximum * percentageUpperBound and frequency[i] >= minOccurences:\n",
    "                newDict.append(dictionary[i])\n",
    "        return newDict\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutualInformationSummand(fSel, fOpp, tSel, tOpp):\n",
    "        p_xy = 1.0 * fSel / (tSel + tOpp)\n",
    "        p_x = (fSel + fOpp) * 1.0 / (tSel + tOpp)\n",
    "        p_y = tSel * 1.0 / (tSel + tOpp)\n",
    "        if p_xy / (p_x * p_y) > 0:\n",
    "            return p_xy * math.log(p_xy / (p_x * p_y))\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutualInformation(dSpam, fSpam, dHam, fHam):\n",
    "        mSpam = []\n",
    "        for i in xrange(len(dSpam)):\n",
    "            if dSpam[i] in dHam:\n",
    "                fOpp = fHam[dHam.index(dSpam[i])]\n",
    "            else:\n",
    "                fOpp = 0\n",
    "            MI = DictionaryUtils.mutualInformationSummand(fSpam[i], fOpp, len(dSpam), len(dHam))\n",
    "            MI += DictionaryUtils.mutualInformationSummand(fOpp, fSpam[i], len(dHam), len(dSpam))\n",
    "            mSpam += [{\"word\" : dSpam[i], \"MI\" : MI}]\n",
    "        for i in xrange(len(dHam)):\n",
    "            if dHam[i] not in dSpam:\n",
    "                MI = DictionaryUtils.mutualInformationSummand(fHam[i], 0, len(dHam), len(dSpam))\n",
    "                mSpam += [{\"word\" : dHam[i], \"MI\" : MI}]\n",
    "        return mSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PickledDataProcessing:\n",
    "    #Expand the database from a given pickled file. Merge every mergeAfter messages.\n",
    "    @staticmethod\n",
    "    def crawlMails(dictionary, frequency, dump, breakAfter = -1, mergeAfter = 10000):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        data = pickle.load(open( dump, \"rb\" ))\n",
    "        files = []\n",
    "        for text in data:\n",
    "            content = (Parser.stripHeaders(text)).lower()\n",
    "            files.append(content)\n",
    "            if i == breakAfter:\n",
    "                break;\n",
    "            if j == mergeAfter:\n",
    "                dictionary, frequency = DictionaryUtils.createDictionary(dictionary, frequency, files)\n",
    "                del(files)\n",
    "                files = []\n",
    "                j = -1\n",
    "            i+=1        \n",
    "            j+=1\n",
    "        dictionary, frequency = DictionaryUtils.createDictionary(dictionary, frequency, files)\n",
    "        return dictionary, frequency\n",
    "    \n",
    "    #Load file from given databases. Return it as an input data for classifier parsed with a given parser.\n",
    "    @staticmethod\n",
    "    def loadMails(parser, pickledFile, count = -1):\n",
    "        msgs = []\n",
    "        ctr = 0\n",
    "        for i in pickle.load(open( pickledFile, \"rb\" )):\n",
    "            msgs.append(parser.parseEmail(i))\n",
    "            if (count != -1):\n",
    "                if ctr == count:\n",
    "                    break\n",
    "                ctr += 1\n",
    "                \n",
    "        return msgs\n",
    "    \n",
    "    #Serialize all mails in given directory into one pickle file of the same name.\n",
    "    @staticmethod\n",
    "    def picklizeDirectory(rootDir):\n",
    "        msgs = []\n",
    "        for filename in os.listdir(rootDir):\n",
    "            with open(rootDir + \"/\" + filename) as f:\n",
    "                msgs.append(f.read().lower())\n",
    "        print \"total # of emails: \" + str(len(msgs))\n",
    "        pickle.dump( msgs, open( rootDir + \".p\", \"wb\" ))\n",
    "        \n",
    "    #Build a dictionary (array) containing all words from given datasets.\n",
    "    @staticmethod\n",
    "    def extractDictionary(pickledData, amount = 2000, dictionarySize = 15000):\n",
    "        d, f = [], []\n",
    "        for x in pickledData:\n",
    "            d, f = PickledDataProcessing.crawlMails(d, f, x, breakAfter = amount)\n",
    "        \n",
    "        if dictionarySize == -1:\n",
    "            return d, f\n",
    "        \n",
    "        topBarrier = 1\n",
    "        botBarrier = 1\n",
    "        sim = DictionaryUtils.simplifyDictionary(d, f, topBarrier, botBarrier)\n",
    "        while len(sim) > dictionarySize and dictionarySize != -1:\n",
    "            topBarrier -= 0.0025\n",
    "            botBarrier += 1\n",
    "            sim = DictionaryUtils.simplifyDictionary(d, f, topBarrier, botBarrier)\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove words that have no effect on the classification process for a given classifier.\n",
    "def optimizeDictionary(dictionary, classifier):\n",
    "    newDict = []\n",
    "    for i in xrange(len(dictionary)):\n",
    "        if classifier.a[i, 0] != 0:\n",
    "            newDict.append(dictionary[i])\n",
    "    return newDict\n",
    "\n",
    "#Create a classifier from two given pickled arrays - ham and spam.\n",
    "def buildClassifier(pickledHam, pickledSpam, parser, amount = 2000):\n",
    "    spam = PickledDataProcessing.loadMails(parser, pickledSpam, amount)\n",
    "    ham = PickledDataProcessing.loadMails(parser, pickledHam, amount)\n",
    "    \n",
    "    classifier = Classifier(spam + ham, [1] * len(spam) + [0] * len(ham))\n",
    "    return classifier\n",
    "\n",
    "#Evaluate the quality of a given classifier on several saved datasets.\n",
    "def evaluateDataset(classifier, parser, pickledDataset, amount = -1):\n",
    "    i = 0\n",
    "    ctr = 0\n",
    "    dataset = pickle.load(open(pickledDataset, \"rb\"))\n",
    "    for x in dataset:\n",
    "        if classifier.evaluate(parser.parseEmail(x)) > 1.0 / 2:\n",
    "            i += 1\n",
    "        ctr += 1\n",
    "        if amount == ctr:\n",
    "            break\n",
    "    print str(100 * i / ctr) + \"% of given datased was classified as positive!\"\n",
    "    \n",
    "def testClassifier(classifier, parser, label = \"\", amount = -1):\n",
    "    print \"***************\"+ label +\"***************\"\n",
    "#    print \"Spamassassin HAM\"\n",
    "#    evaluateDataset(classifier, parser, \"ham.p\", amount)\n",
    "#    gc.collect()\n",
    "#    print \"Spamassassin SPAM\"\n",
    "#    evaluateDataset(classifier, parser, \"spam.p\", amount)\n",
    "#    gc.collect()\n",
    "    print \"Enron HAM\"\n",
    "    evaluateDataset(classifier, parser, \"Enron/ham.p\", amount)\n",
    "    gc.collect()\n",
    "    print \"Enron SPAM\"\n",
    "    evaluateDataset(classifier, parser, \"Enron/spam.p\", amount)\n",
    "    gc.collect()\n",
    "    print \"SPAM dataset\"\n",
    "    evaluateDataset(classifier, parser, \"SPAM/01.p\")\n",
    "    gc.collect()\n",
    "    print \"Classifier evaluation DONE\"\n",
    "    print \"***************\"+ label +\"***************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import resource\n",
    "\n",
    "soft, hard = 5.0 * 10**9, 5.0 * 10**9\n",
    "resource.setrlimit(resource.RLIMIT_AS,(soft, hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies/Dictionaries loaded\n",
      "Dictionary Impact-sorting done\n",
      "Dictionary Export done, length 26661\n"
     ]
    }
   ],
   "source": [
    "dHam, fHam = PickledDataProcessing.extractDictionary([\"Enron/ham.p\"], 500, -1)\n",
    "dSpam, fSpam = PickledDataProcessing.extractDictionary([\"Enron/spam.p\"], 500, -1)\n",
    "print \"Frequencies/Dictionaries loaded\"\n",
    "\n",
    "MI = DictionaryUtils.mutualInformation(dSpam, fSpam, dHam, fHam)\n",
    "sMI = sorted(MI, key=lambda k: -k['MI']) \n",
    "print \"Dictionary Impact-sorting done\"\n",
    "sDict = [i['word'] for i in sMI]\n",
    "print \"Dictionary Export done, length \" + str(len(sDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimziedDictionary = sDict[0:250]\n",
    "parser = Parser(optimziedDictionary)\n",
    "#classifier = buildClassifier(\"Enron/ham.p\", \"Enron/spam.p\", parser, amount = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared spam/ham\n",
      "***************Enron dataset***************\n",
      "Enron HAM\n",
      "11% of given datased was classified as positive!\n",
      "Enron SPAM\n",
      "89% of given datased was classified as positive!\n",
      "SPAM dataset\n",
      "96% of given datased was classified as positive!\n",
      "Classifier evaluation DONE\n",
      "***************Enron dataset***************\n"
     ]
    }
   ],
   "source": [
    "spam = PickledDataProcessing.loadMails(parser, \"Enron/spam.p\", 500)\n",
    "ham = PickledDataProcessing.loadMails(parser, \"Enron/ham.p\", 500)\n",
    "print \"Prepared spam/ham\"\n",
    "classifier = Classifier(spam + ham, [1] * len(spam) + [0] * len(ham))\n",
    "testClassifier(classifier, parser, \"Enron dataset\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
